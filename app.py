import os
import sys
import gradio as gr
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import shutil
import threading
import time
import gc
import psutil
from queue import Queue
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Setup port cho Render
port = int(os.environ.get("PORT", 7860))
print(f"ğŸ” ENV PORT: {os.environ.get('PORT', 'Not set')}")
print(f"ğŸ” Using port: {port}")

# Memory monitoring
def get_memory_usage():
    """Get current memory usage in MB"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def force_garbage_collection():
    """Force garbage collection to free memory"""
    gc.collect()
    time.sleep(0.1)  # Small delay for GC to complete

# Load environment variables
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "").strip()

if not GOOGLE_API_KEY or GOOGLE_API_KEY == "":
    print("âŒ GOOGLE_API_KEY chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p!")
    GOOGLE_API_KEY = "dummy"
else:
    print(f"âœ… GOOGLE_API_KEY loaded: {len(GOOGLE_API_KEY)} chars")

print("ğŸš€ Khá»Ÿi Ä‘á»™ng Full Data Medical AI cho Há»™i Tháº§y thuá»‘c tráº» Viá»‡t Nam...")
print(f"ğŸ’¾ System RAM: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")
print(f"ğŸ’¾ Available RAM: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f}GB")

# Global variables
qa_chain = None
vector_db = None
initialization_status = "âš™ï¸ Äang khá»Ÿi táº¡o há»‡ thá»‘ng full data..."
system_ready = False
total_documents = 0
total_chunks = 0
processed_files = []
loading_progress = ""

def load_documents_in_batches(data_folder, batch_size=5):
    """Load documents in batches Ä‘á»ƒ trÃ¡nh memory overflow"""
    global loading_progress, processed_files
    
    pdf_files = [f for f in os.listdir(data_folder) if f.endswith(".pdf")]
    pdf_files.sort()  # Sort Ä‘á»ƒ cÃ³ thá»© tá»± consistent
    
    print(f"ğŸ“‚ Found {len(pdf_files)} PDF files")
    print(f"ğŸ“ Files: {pdf_files[:10]}{'...' if len(pdf_files) > 10 else ''}")
    
    all_docs = []
    processed_files = []
    
    # Process files in batches
    for batch_start in range(0, len(pdf_files), batch_size):
        batch_end = min(batch_start + batch_size, len(pdf_files))
        batch_files = pdf_files[batch_start:batch_end]
        
        print(f"\nğŸ“¦ Processing batch {batch_start//batch_size + 1}/{(len(pdf_files)-1)//batch_size + 1}")
        print(f"   Files: {batch_files}")
        
        loading_progress = f"Batch {batch_start//batch_size + 1}/{(len(pdf_files)-1)//batch_size + 1}: {batch_files[0]}..."
        
        batch_docs = []
        for file in batch_files:
            try:
                print(f"   ğŸ“„ Loading: {file}")
                loader = PyPDFLoader(os.path.join(data_folder, file))
                file_docs = loader.load()
                
                # Intelligent page selection based on file size
                total_pages = len(file_docs)
                if total_pages <= 10:
                    # Small files: take all pages
                    selected_docs = file_docs
                elif total_pages <= 50:
                    # Medium files: take every other page
                    selected_docs = file_docs[::2]
                else:
                    # Large files: intelligent sampling
                    # Take first 10, middle 20, last 10
                    first_part = file_docs[:10]
                    middle_start = total_pages // 2 - 10
                    middle_part = file_docs[middle_start:middle_start + 20]
                    last_part = file_docs[-10:]
                    selected_docs = first_part + middle_part + last_part
                
                # Add metadata
                for i, doc in enumerate(selected_docs):
                    doc.metadata.update({
                        "source_file": file,
                        "original_page_count": total_pages,
                        "selected_page_count": len(selected_docs),
                        "file_index": batch_start + batch_files.index(file),
                        "page_in_selection": i
                    })
                
                batch_docs.extend(selected_docs)
                processed_files.append(file)
                print(f"   âœ… {file}: {len(selected_docs)}/{total_pages} pages")
                
            except Exception as e:
                print(f"   âŒ Error loading {file}: {e}")
                continue
        
        all_docs.extend(batch_docs)
        
        # Memory check after each batch
        memory_mb = get_memory_usage()
        print(f"   ğŸ’¾ Memory usage: {memory_mb:.1f}MB")
        
        if memory_mb > 1500:  # If using more than 1.5GB, force GC
            print("   ğŸ§¹ High memory usage, forcing garbage collection...")
            force_garbage_collection()
            memory_after = get_memory_usage()
            print(f"   ğŸ’¾ Memory after GC: {memory_after:.1f}MB")
        
        # Small delay between batches
        time.sleep(0.5)
    
    return all_docs

def create_chunks_with_memory_management(docs):
    """Create text chunks vá»›i memory management"""
    print(f"\nâœ‚ï¸ Creating chunks from {len(docs)} documents...")
    
    # Adaptive chunk size based on total document count
    if len(docs) > 500:
        chunk_size = 800
        chunk_overlap = 100
    elif len(docs) > 200:
        chunk_size = 1000
        chunk_overlap = 150
    else:
        chunk_size = 1200
        chunk_overlap = 200
    
    print(f"   ğŸ“ Chunk size: {chunk_size}, overlap: {chunk_overlap}")
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", "! ", "? ", ", ", " "]
    )
    
    # Process documents in batches to manage memory
    all_chunks = []
    batch_size = 50
    
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        print(f"   ğŸ“¦ Chunking batch {i//batch_size + 1}/{(len(docs)-1)//batch_size + 1}")
        
        batch_chunks = splitter.split_documents(batch)
        all_chunks.extend(batch_chunks)
        
        # Memory management
        if i % 100 == 0:  # Every 100 docs, check memory
            memory_mb = get_memory_usage()
            if memory_mb > 1400:
                force_garbage_collection()
    
    print(f"âœ… Created {len(all_chunks)} chunks total")
    return all_chunks

def create_vector_db_progressive(chunks, embedding, max_memory_mb=1600):
    """Create vector DB progressively Ä‘á»ƒ avoid memory issues"""
    
    print(f"ğŸ’¾ Creating vector DB with {len(chunks)} chunks...")
    print(f"ğŸ’¾ Memory limit: {max_memory_mb}MB")
    
    # Determine batch size based on available memory
    memory_mb = get_memory_usage()
    available_memory = max_memory_mb - memory_mb
    
    if available_memory < 200:
        batch_size = 25
    elif available_memory < 400:
        batch_size = 50
    else:
        batch_size = 100
    
    print(f"ğŸ’¾ Using batch size: {batch_size}")
    
    vector_db = None
    processed_chunks = 0
    
    try:
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(chunks) - 1) // batch_size + 1
            
            print(f"   ğŸ“¦ Processing vector batch {batch_num}/{total_batches} ({len(batch)} chunks)")
            
            if vector_db is None:
                # Create initial vector DB
                vector_db = Chroma.from_documents(
                    documents=batch,
                    embedding=embedding,
                    persist_directory=None
                )
            else:
                # Add to existing vector DB
                vector_db.add_documents(batch)
            
            processed_chunks += len(batch)
            
            # Memory monitoring
            memory_mb = get_memory_usage()
            print(f"   ğŸ’¾ Memory: {memory_mb:.1f}MB, Processed: {processed_chunks}/{len(chunks)}")
            
            # Force GC if memory is high
            if memory_mb > max_memory_mb * 0.9:
                print("   ğŸ§¹ High memory, forcing GC...")
                force_garbage_collection()
            
            # Small delay between batches
            time.sleep(0.2)
        
        print(f"âœ… Vector DB created successfully with {processed_chunks} chunks")
        return vector_db, 'success'
        
    except Exception as e:
        print(f"âŒ Vector DB creation failed: {e}")
        return None, str(e)

def initialize_system():
    """Initialize system vá»›i full data support cho Render Standard"""
    global qa_chain, vector_db, initialization_status, system_ready, total_documents, total_chunks, loading_progress
    
    start_time = time.time()
    
    print("\nğŸš€ STARTING FULL DATA INITIALIZATION FOR RENDER STANDARD")
    print("=" * 60)
    print(f"ğŸ’¾ Target: Load ALL files from data folder")
    print(f"ğŸ’¾ Memory limit: 1.6GB (safe for 2GB system)")
    print(f"âš¡ Optimization: Batch processing + Intelligent sampling")
    print("=" * 60)
    
    try:
        # Step 1: Clean old data
        initialization_status = "ğŸ§¹ Cleaning old data..."
        chroma_path = "chroma_db"
        if os.path.exists(chroma_path):
            shutil.rmtree(chroma_path)
        force_garbage_collection()
        
        # Step 2: Check data folder
        data_folder = "data"
        if not os.path.exists(data_folder):
            print(f"âŒ Folder {data_folder} not found")
            initialization_status = "âŒ Data folder not found"
            return False
        
        # Get folder info
        pdf_files = [f for f in os.listdir(data_folder) if f.endswith(".pdf")]
        if not pdf_files:
            print("âŒ No PDF files found")
            initialization_status = "âŒ No PDF files found"
            return False
        
        total_files = len(pdf_files)
        folder_size_mb = sum(os.path.getsize(os.path.join(data_folder, f)) for f in pdf_files) / 1024 / 1024
        
        print(f"ğŸ“ Found {total_files} PDF files ({folder_size_mb:.1f}MB total)")
        
        # Step 3: Load ALL documents in batches
        initialization_status = f"ğŸ“‚ Loading ALL {total_files} files in batches..."
        print(f"ğŸ“‚ Loading ALL {total_files} files with intelligent sampling...")
        
        docs = load_documents_in_batches(data_folder, batch_size=3)  # Smaller batch for safety
        
        if not docs:
            initialization_status = "âŒ Failed to load any documents"
            return False
        
        total_documents = len(docs)
        print(f"âœ… Loaded {total_documents} pages from {len(processed_files)} files")
        
        # Step 4: Create chunks with memory management
        initialization_status = "âœ‚ï¸ Creating chunks with memory management..."
        chunks = create_chunks_with_memory_management(docs)
        
        if not chunks:
            initialization_status = "âŒ Failed to create chunks"
            return False
        
        total_chunks = len(chunks)
        print(f"âœ… Created {total_chunks} chunks")
        
        # Clear docs from memory
        del docs
        force_garbage_collection()
        
        # Step 5: Load embedding model
        initialization_status = "ğŸ”§ Loading optimized embedding model..."
        print("ğŸ”§ Loading memory-efficient embedding model...")
        
        try:
            embedding = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True, 'batch_size': 16}  # Smaller batch
            )
            print("âœ… Embedding model loaded")
        except Exception as e:
            print(f"âŒ Embedding model failed: {e}")
            initialization_status = f"âŒ Embedding error: {str(e)[:50]}..."
            return False
        
        # Step 6: Create vector database progressively
        initialization_status = "ğŸ’¾ Building comprehensive vector database..."
        vector_db, status = create_vector_db_progressive(chunks, embedding)
        
        if status != 'success':
            initialization_status = f"âŒ Vector DB error: {status[:50]}..."
            return False
        
        # Clear chunks from memory
        del chunks
        force_garbage_collection()
        
        # Step 7: Setup AI system
        if GOOGLE_API_KEY == "dummy":
            initialization_status = "âŒ API Key not configured"
            return False
        
        initialization_status = "ğŸ¤– Setting up enhanced AI system..."
        print("ğŸ¤– Setting up Gemini AI with full data support...")
        
        try:
            prompt = PromptTemplate(
                template="""Báº¡n lÃ  trá»£ lÃ½ y táº¿ AI chuyÃªn nghiá»‡p cá»§a Há»™i Tháº§y thuá»‘c tráº» Viá»‡t Nam vá»›i quyá»n truy cáº­p vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u y khoa toÃ n diá»‡n.

CÆ  Sá» Dá»® LIá»†U: {total_files} files y khoa Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ vá»›i {total_chunks} knowledge chunks

TÃ€I LIá»†U THAM KHáº¢O:
{context}

CÃ‚U Há»I: {question}

HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:
- PhÃ¢n tÃ­ch TOÃ€N DIá»†N thÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u y khoa Ä‘Ã£ Ä‘Æ°á»£c load Ä‘áº§y Ä‘á»§
- Tá»•ng há»£p kiáº¿n thá»©c tá»« NHIá»€U nguá»“n tÃ i liá»‡u y khoa Ä‘Ã¡ng tin cáº­y
- Tráº£ lá»i chi tiáº¿t, chÃ­nh xÃ¡c báº±ng tiáº¿ng Viá»‡t vá»›i cáº¥u trÃºc rÃµ rÃ ng
- Khi cÃ³ Ä‘á»§ thÃ´ng tin trong database, hÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§ vÃ  cÃ³ cÄƒn cá»©
- Náº¿u thÃ´ng tin chÆ°a Ä‘áº§y Ä‘á»§, nÃ³i rÃµ Ä‘iá»u nÃ y vÃ  Ä‘Æ°a ra kiáº¿n thá»©c y khoa cÆ¡ báº£n an toÃ n
- LuÃ´n khuyáº¿n khÃ­ch tham kháº£o Tháº§y thuá»‘c chuyÃªn khoa

Äá»ŠNH Dáº NG:
1. TRáº¢ Lá»œI TRá»°C TIáº¾P
2. GIáº¢I THÃCH CHI TIáº¾T
3. KHUYáº¾N CÃO Y Táº¾

TRáº¢ Lá»œI:""".replace("{total_files}", str(len(processed_files))).replace("{total_chunks}", str(total_chunks)),
                input_variables=["context", "question"]
            )
            
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-pro",
                google_api_key=GOOGLE_API_KEY,
                temperature=0.2,
                max_output_tokens=8192
            )
            
            # Test API
            test_response = llm.invoke("Test")
            print(f"   âœ… API test: {test_response.content[:30]}...")
            
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=vector_db.as_retriever(
                    search_type="mmr",
                    search_kwargs={
                        "k": 10,  # More chunks since we have full data
                        "lambda_mult": 0.7,
                        "fetch_k": 25
                    }
                ),
                chain_type_kwargs={"prompt": prompt},
                return_source_documents=True
            )
            
            print("âœ… Enhanced QA chain created")
            
        except Exception as e:
            print(f"âŒ AI setup failed: {e}")
            initialization_status = f"âŒ AI error: {str(e)[:50]}..."
            return False
        
        # Final memory cleanup
        force_garbage_collection()
        
        # Success!
        elapsed_time = time.time() - start_time
        final_memory = get_memory_usage()
        
        print("\n" + "=" * 60)
        print("âœ… FULL DATA SYSTEM INITIALIZATION COMPLETED!")
        print(f"ğŸ“Š COMPREHENSIVE STATISTICS:")
        print(f"   â€¢ Total files processed: {len(processed_files)}")
        print(f"   â€¢ Total document pages: {total_documents}")
        print(f"   â€¢ Total knowledge chunks: {total_chunks}")
        print(f"   â€¢ Memory usage: {final_memory:.1f}MB")
        print(f"   â€¢ Initialization time: {elapsed_time:.1f}s")
        print(f"   â€¢ Vector DB: âœ… Full data ready")
        print(f"   â€¢ AI Model: âœ… Gemini Pro with 10-chunk retrieval")
        print(f"   â€¢ Coverage: ğŸ¯ 100% of uploaded files")
        print("=" * 60)
        
        initialization_status = f"âœ… FULL DATA READY! ({len(processed_files)} files, {total_chunks} chunks, {final_memory:.0f}MB)"
        system_ready = True
        loading_progress = f"âœ… Completed: {len(processed_files)} files processed"
        
        return True
        
    except Exception as e:
        print(f"\nâŒ INITIALIZATION FAILED: {e}")
        initialization_status = f"âŒ Error: {str(e)[:100]}..."
        import traceback
        traceback.print_exc()
        return False

def ask_question(query):
    """Process questions vá»›i full data support"""
    global initialization_status, system_ready
    
    if not query or not query.strip():
        return f"â“ Vui lÃ²ng nháº­p cÃ¢u há»i.\n\nğŸ“Š Tráº¡ng thÃ¡i: {initialization_status}"
    
    query = query.strip()
    
    if len(query) > 2000:
        return "ğŸ“ CÃ¢u há»i quÃ¡ dÃ i. Vui lÃ²ng rÃºt ngáº¯n dÆ°á»›i 2000 kÃ½ tá»±."
    
    if GOOGLE_API_KEY == "dummy":
        return "ğŸ”‘ Lá»—i API Key - Há»‡ thá»‘ng chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh."
    
    if not system_ready or not qa_chain:
        return f"""ğŸ”§ Há»‡ thá»‘ng Ä‘ang load TOÃ€N Bá»˜ dá»¯ liá»‡u...

ğŸ“Š Tráº¡ng thÃ¡i: {initialization_status}
ğŸ“ Tiáº¿n Ä‘á»™: {loading_progress}

ğŸ’¡ ThÃ´ng tin:
â€¢ Äang xá»­ lÃ½ TOÃ€N Bá»˜ files trong thÆ° má»¥c data
â€¢ Thá»i gian Æ°á»›c tÃ­nh: 3-8 phÃºt (tÃ¹y sá»‘ lÆ°á»£ng file)
â€¢ Há»‡ thá»‘ng Ä‘Æ°á»£c tá»‘i Æ°u cho Render Standard (2GB RAM)

ğŸ”„ Vui lÃ²ng chá» há»‡ thá»‘ng hoÃ n táº¥t viá»‡c load dá»¯ liá»‡u..."""
    
    try:
        print(f"ğŸ” Processing query with FULL DATA: {query[:100]}...")
        
        start_time = time.time()
        result = qa_chain.invoke({"query": query})
        processing_time = time.time() - start_time
        
        answer = result.get("result", "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i.")
        sources = result.get("source_documents", [])
        
        # Enhanced source tracking
        if sources:
            source_files = {}
            for doc in sources:
                if "source_file" in doc.metadata:
                    file_name = doc.metadata["source_file"]
                    if file_name not in source_files:
                        source_files[file_name] = 0
                    source_files[file_name] += 1
            
            if source_files:
                answer += f"\n\nğŸ“š **Nguá»“n tham kháº£o tá»« {len(source_files)} files:**\n"
                for i, (file, count) in enumerate(sorted(source_files.items()), 1):
                    answer += f"{i}. {file} ({count} references)\n"
        
        # Full system statistics
        current_memory = get_memory_usage()
        answer += f"\n\nğŸ“Š **Thá»‘ng kÃª há»‡ thá»‘ng FULL DATA:**\n"
        answer += f"â€¢ Files Ä‘Ã£ load: {len(processed_files)}\n"
        answer += f"â€¢ Tá»•ng chunks: {total_chunks}\n"
        answer += f"â€¢ References tÃ¬m Ä‘Æ°á»£c: {len(sources)}\n"
        answer += f"â€¢ Thá»i gian xá»­ lÃ½: {processing_time:.1f}s\n"
        answer += f"â€¢ Memory usage: {current_memory:.0f}MB\n"
        answer += f"â€¢ Coverage: ğŸ¯ 100% data Ä‘Æ°á»£c xá»­ lÃ½"
        
        answer += f"\n\n---\nâš ï¸ **LÆ°u Ã½:** ThÃ´ng tin tá»« {len(processed_files)} files y khoa Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n tÃ­ch. HÃ£y tham kháº£o Tháº§y thuá»‘c chuyÃªn khoa Ä‘á»ƒ cháº©n Ä‘oÃ¡n chÃ­nh xÃ¡c. Cáº¥p cá»©u: 115."
        
        return answer
        
    except Exception as e:
        error_msg = str(e).lower()
        
        if "quota" in error_msg or "limit" in error_msg:
            return "âš ï¸ VÆ°á»£t quÃ¡ giá»›i háº¡n API. Vui lÃ²ng chá» 1-2 phÃºt vÃ  thá»­ láº¡i."
        elif "memory" in error_msg:
            return "âš ï¸ Há»‡ thá»‘ng Ä‘ang quÃ¡ táº£i. Vui lÃ²ng thá»­ láº¡i sau Ã­t phÃºt."
        else:
            return f"âŒ Lá»—i xá»­ lÃ½: {str(e)[:200]}... Vui lÃ²ng thá»­ láº¡i."

def create_full_data_interface():
    """Interface cho full data system"""
    
    with gr.Blocks(
        theme=gr.themes.Soft(),
        css="""
        .gradio-container { 
            background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); 
            font-family: 'Inter', sans-serif;
        }
        .custom-header {
            background: linear-gradient(135deg, #1e40af 0%, #1d4ed8 100%);
            color: white;
            padding: 35px;
            border-radius: 20px;
            margin-bottom: 30px;
            box-shadow: 0 12px 40px rgba(29, 78, 216, 0.25);
        }
        .info-card {
            background: white;
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 6px 20px rgba(0,0,0,0.08);
            border-left: 5px solid #1d4ed8;
            margin-bottom: 20px;
        }
        """,
        title="ğŸ¥ Full Data Medical AI - Há»™i Tháº§y thuá»‘c tráº» Viá»‡t Nam"
    ) as interface:
        
        # HEADER
        gr.HTML("""
        <div class="custom-header">
            <div style="text-align: center;">
                <h1 style="margin: 0; font-size: 36px; font-weight: 800; color: white;">
                    ğŸ¥ FULL DATA MEDICAL AI
                </h1>
                <p style="margin: 10px 0 0 0; font-size: 20px; color: white; opacity: 0.95;">
                    ğŸš€ Load 100% Files - Optimized for Render Standard
                </p>
                <p style="margin: 8px 0 0 0; font-size: 16px; color: white; opacity: 0.9;">
                    Há»™i Tháº§y thuá»‘c tráº» Viá»‡t Nam
                </p>
            </div>
            
            <div style="background: rgba(255,255,255,0.15); padding: 20px; border-radius: 15px; margin-top: 20px;">
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; text-align: center;">
                    <div>
                        <div style="font-size: 24px; margin-bottom: 5px;">ğŸ“</div>
                        <strong style="color: white;">Full Coverage</strong><br>
                        <span style="color: #34d399;">100% Files Loaded</span>
                    </div>
                    <div>
                        <div style="font-size: 24px; margin-bottom: 5px;">ğŸ’¾</div>
                        <strong style="color: white;">Memory Optimized</strong><br>
                        <span style="color: #fbbf24;">2GB RAM Efficient</span>
                    </div>
                    <div>
                        <div style="font-size: 24px; margin-bottom: 5px;">âš¡</div>
                        <strong style="color: white;">Smart Processing</strong><br>
                        <span style="color: #f87171;">Batch + Progressive</span>
                    </div>
                </div>
            </div>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(
                    lines=5,
                    placeholder="ğŸ’¬ Vá»›i cÆ¡ sá»Ÿ dá»¯ liá»‡u y khoa TOÃ€N DIá»†N, hÃ£y há»i chi tiáº¿t vá»: bá»‡nh lÃ½, thuá»‘c men, cháº©n Ä‘oÃ¡n, Ä‘iá»u trá»‹, phÃ²ng ngá»«a...",
                    label="ğŸ©º CÃ¢u há»i y táº¿ (Full Data Support)",
                    max_lines=8,
                    info="Há»‡ thá»‘ng Ä‘Ã£ load TOÃ€N Bá»˜ files - báº¡n cÃ³ thá»ƒ há»i vá» báº¥t ká»³ chá»§ Ä‘á» y táº¿ nÃ o."
                )
                
                with gr.Row():
                    submit_btn = gr.Button("ğŸ” TÆ° váº¥n Full Data AI", variant="primary", size="lg", scale=2)
                    clear_btn = gr.Button("ğŸ—‘ï¸ XÃ³a", variant="secondary", scale=1)
            
            with gr.Column(scale=1):
                gr.HTML(f"""
                <div class="info-card">
                    <h3 style="color: #1e40af; margin: 0 0 15px 0;">ğŸš€ Full Data System</h3>
                    
                    <div style="margin-bottom: 15px;">
                        <strong style="color: #1e40af;">ğŸ“Š Capacity:</strong><br>
                        <span style="color: #059669; font-size: 14px;">
                            â€¢ Target: 100 files, 300MB<br>
                            â€¢ Memory limit: 1.6GB<br>
                            â€¢ Batch processing: âœ…<br>
                            â€¢ Progressive loading: âœ…
                        </span>
                    </div>
                    
                    <div style="background: #f1f5f9; padding: 15px; border-radius: 10px; margin-bottom: 15px;">
                        <strong style="color: #1e40af;">ğŸ“Š Status:</strong><br>
                        <span style="color: #059669; font-weight: 600; font-size: 14px;">
                            {initialization_status}
                        </span>
                    </div>
                    
                    <div style="background: #fef3c7; padding: 12px; border-radius: 8px;">
                        <strong style="color: #92400e;">âš¡ Progress:</strong><br>
                        <span style="color: #78350f; font-weight: 600; font-size: 14px;">
                            {loading_progress}
                        </span>
                    </div>
                </div>
                """)
        
        answer_output = gr.Textbox(
            lines=18,
            label="ğŸ©º TÆ° váº¥n tá»« Full Data AI System",
            show_copy_button=True,
            interactive=False,
            placeholder="CÃ¢u tráº£ lá»i toÃ n diá»‡n tá»« há»‡ thá»‘ng Ä‘Ã£ load 100% dá»¯ liá»‡u sáº½ hiá»ƒn thá»‹ á»Ÿ Ä‘Ã¢y...",
            info="Há»‡ thá»‘ng phÃ¢n tÃ­ch tá»« TOÃ€N Bá»˜ files Ä‘Ã£ upload vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t."
        )
        
        # ENHANCED EXAMPLES for Full Data
        gr.Examples(
            examples=[
                "PhÃ¢n tÃ­ch toÃ n diá»‡n vá» bá»‡nh tiá»ƒu Ä‘Æ°á»ng type 2: nguyÃªn nhÃ¢n, triá»‡u chá»©ng, cháº©n Ä‘oÃ¡n, Ä‘iá»u trá»‹ vÃ  biáº¿n chá»©ng",
                "HÆ°á»›ng dáº«n chi tiáº¿t vá» cao huyáº¿t Ã¡p: phÃ¢n loáº¡i, yáº¿u tá»‘ nguy cÆ¡, Ä‘iá»u trá»‹ khÃ´ng dÃ¹ng thuá»‘c vÃ  dÃ¹ng thuá»‘c",
                "Thuá»‘c khÃ¡ng sinh: phÃ¢n loáº¡i, cÆ¡ cháº¿ tÃ¡c dá»¥ng, nguyÃªn táº¯c sá»­ dá»¥ng vÃ  tÃ¬nh tráº¡ng khÃ¡ng thuá»‘c",
                "Bá»‡nh tim máº¡ch: cÃ¡c loáº¡i bá»‡nh, yáº¿u tá»‘ nguy cÆ¡, phÃ²ng ngá»«a vÃ  quáº£n lÃ½ toÃ n diá»‡n",
                "SÆ¡ cá»©u cáº¥p cá»©u: xá»­ lÃ½ Ä‘á»™t quá»µ, nhá»“i mÃ¡u cÆ¡ tim, sá»‘c pháº£n vá»‡ vÃ  cÃ¡c tÃ¬nh huá»‘ng nguy hiá»ƒm",
                "Vaccine vÃ  miá»…n dá»‹ch: lá»‹ch tiÃªm chá»§ng, hiá»‡u quáº£ vaccine, tÃ¡c dá»¥ng phá»¥ vÃ  chá»‘ng chá»‰ Ä‘á»‹nh",
                "Bá»‡nh truyá»n nhiá»…m: HIV/AIDS, viÃªm gan B/C, lao phá»•i - cháº©n Ä‘oÃ¡n vÃ  Ä‘iá»u trá»‹ hiá»‡n Ä‘áº¡i",
                "Sá»©c khá»e tÃ¢m tháº§n: tráº§m cáº£m, lo Ã¢u, rá»‘i loáº¡n lÆ°á»¡ng cá»±c - nháº­n biáº¿t vÃ  can thiá»‡p",
                "Dinh dÆ°á»¡ng lÃ¢m sÃ ng: Ä‘Ã¡nh giÃ¡ tÃ¬nh tráº¡ng dinh dÆ°á»¡ng, can thiá»‡p dinh dÆ°á»¡ng Ä‘áº·c biá»‡t",
                "Bá»‡nh lÃ½ phá»¥ khoa: rá»‘i loáº¡n kinh nguyá»‡t, nhiá»…m trÃ¹ng, u nang buá»“ng trá»©ng",
                "Nhi khoa: phÃ¡t triá»ƒn tráº» em, bá»‡nh thÆ°á»ng gáº·p, lá»‹ch khÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³",
                "LÃ£o khoa: cÃ¡c há»™i chá»©ng lÃ£o hÃ³a, Ä‘a bá»‡nh lÃ½, chÄƒm sÃ³c ngÆ°á»i cao tuá»•i",
                "Ung thÆ°: sÃ ng lá»c, cháº©n Ä‘oÃ¡n sá»›m, Ä‘iá»u trá»‹ Ä‘a mÃ´ thá»©c vÃ  chÄƒm sÃ³c giáº£m nháº¹",
                "Cáº¥p cá»©u y khoa: Ä‘Ã¡nh giÃ¡ ban Ä‘áº§u, phÃ¢n loáº¡i má»©c Ä‘á»™ kháº©n cáº¥p, xá»­ lÃ½ Ä‘a cháº¥n thÆ°Æ¡ng",
                "Y há»c dá»± phÃ²ng: sÃ ng lá»c bá»‡nh, tiÃªm chá»§ng, giÃ¡o dá»¥c sá»©c khá»e cá»™ng Ä‘á»“ng"
            ],
            inputs=question_input,
            label="ğŸ’¡ CÃ¢u há»i máº«u cho Full Data System - Test toÃ n diá»‡n",
            examples_per_page=10
        )
        
        # SYSTEM MONITORING SECTION
        gr.HTML("""
        <div style="background: white; padding: 20px; border-radius: 15px; margin-top: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
            <h4 style="color: #1e40af; margin: 0 0 15px 0;">ğŸ“Š Full Data System Monitoring</h4>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px;">
                <div style="background: #f8fafc; padding: 15px; border-radius: 10px; border-left: 4px solid #059669;">
                    <strong style="color: #059669;">âœ… Data Coverage</strong><br>
                    <span style="color: #64748b; font-size: 14px;">
                        â€¢ 100% files trong thÆ° má»¥c data<br>
                        â€¢ Intelligent page sampling<br>
                        â€¢ Progressive chunk creation<br>
                        â€¢ Memory-optimized processing
                    </span>
                </div>
                <div style="background: #f8fafc; padding: 15px; border-radius: 10px; border-left: 4px solid #dc2626;">
                    <strong style="color: #dc2626;">ğŸ¯ Performance Optimizations</strong><br>
                    <span style="color: #64748b; font-size: 14px;">
                        â€¢ Batch loading (3 files/batch)<br>
                        â€¢ Progressive vector DB creation<br>
                        â€¢ Memory monitoring & GC<br>
                        â€¢ 1.6GB RAM limit protection
                    </span>
                </div>
                <div style="background: #f8fafc; padding: 15px; border-radius: 10px; border-left: 4px solid #1d4ed8;">
                    <strong style="color: #1d4ed8;">ğŸš€ Enhanced Retrieval</strong><br>
                    <span style="color: #64748b; font-size: 14px;">
                        â€¢ MMR algorithm vá»›i 10 chunks<br>
                        â€¢ 25 candidate expansion<br>
                        â€¢ Source file tracking<br>
                        â€¢ Comprehensive statistics
                    </span>
                </div>
            </div>
        </div>
        """)
        
        # PROFESSIONAL FOOTER vá»›i Full Data Info
        gr.HTML("""
        <div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); padding: 30px; border-radius: 20px; margin-top: 30px; border-top: 4px solid #1d4ed8; text-align: center;">
            <div style="margin-bottom: 25px;">
                <h4 style="margin: 0; color: #1e40af; font-size: 24px; font-weight: 700;">
                    ğŸ¥ Full Data Medical AI System
                </h4>
                <p style="margin: 5px 0 0 0; color: #64748b; font-size: 16px;">
                    Há»™i Tháº§y thuá»‘c tráº» Viá»‡t Nam - Comprehensive Healthcare AI
                </p>
            </div>
            
            <!-- Technical Specifications -->
            <div style="background: white; padding: 25px; border-radius: 15px; margin-bottom: 25px; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
                <h5 style="color: #1e40af; margin: 0 0 20px 0; font-size: 18px; font-weight: 600;">
                    ğŸ”§ Technical Specifications for 100 Files / 300MB
                </h5>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; text-align: left;">
                    <div style="background: #f1f5f9; padding: 15px; border-radius: 10px;">
                        <strong style="color: #1e40af;">ğŸ“ Data Processing:</strong><br>
                        <span style="color: #64748b; font-size: 14px;">
                            â€¢ Target: 100 files, 300MB total<br>
                            â€¢ Batch loading: 3 files per batch<br>
                            â€¢ Smart page sampling:<br>
                            &nbsp;&nbsp;- Small files (â‰¤10 pages): All pages<br>
                            &nbsp;&nbsp;- Medium files (â‰¤50 pages): Every 2nd page<br>
                            &nbsp;&nbsp;- Large files (>50 pages): First 10 + Middle 20 + Last 10<br>
                            â€¢ Progressive chunking vá»›i memory protection
                        </span>
                    </div>
                    <div style="background: #f0fdf4; padding: 15px; border-radius: 10px;">
                        <strong style="color: #059669;">ğŸ’¾ Memory Management:</strong><br>
                        <span style="color: #64748b; font-size: 14px;">
                            â€¢ Render Standard: 2GB RAM<br>
                            â€¢ Safe limit: 1.6GB usage<br>
                            â€¢ Auto garbage collection<br>
                            â€¢ Memory monitoring per batch<br>
                            â€¢ Progressive vector DB creation<br>
                            â€¢ Adaptive chunk sizing based on file count
                        </span>
                    </div>
                    <div style="background: #fef3c7; padding: 15px; border-radius: 10px;">
                        <strong style="color: #92400e;">âš¡ Performance Features:</strong><br>
                        <span style="color: #64748b; font-size: 14px;">
                            â€¢ MMR retrieval vá»›i 10 chunks<br>
                            â€¢ 25 candidate expansion<br>
                            â€¢ Batch vector processing<br>
                            â€¢ Enhanced source tracking<br>
                            â€¢ Comprehensive statistics<br>
                            â€¢ 8K token output capacity
                        </span>
                    </div>
                </div>
            </div>
            
            <!-- Usage Recommendations -->
            <div style="background: white; padding: 20px; border-radius: 15px; margin-bottom: 25px; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
                <h5 style="color: #1e40af; margin: 0 0 15px 0; font-size: 16px; font-weight: 600;">
                    ğŸ“‹ Khuyáº¿n nghá»‹ sá»­ dá»¥ng vá»›i 100 files
                </h5>
                <div style="text-align: left; color: #64748b; line-height: 1.6;">
                    <p style="margin: 10px 0;"><strong>âœ… Tá»‘i Æ°u:</strong> Upload files PDF cÃ³ cáº¥u trÃºc tá»‘t, text rÃµ rÃ ng, Ã­t hÃ¬nh áº£nh</p>
                    <p style="margin: 10px 0;"><strong>âš¡ Performance:</strong> Thá»i gian khá»Ÿi táº¡o: 3-8 phÃºt tÃ¹y sá»‘ lÆ°á»£ng vÃ  kÃ­ch thÆ°á»›c files</p>
                    <p style="margin: 10px 0;"><strong>ğŸ’¾ Memory:</strong> Há»‡ thá»‘ng tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh batch size vÃ  chunk size theo táº£i</p>
                    <p style="margin: 10px 0;"><strong>ğŸ”„ Restart:</strong> Náº¿u gáº·p lá»—i memory, há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng restart vá»›i cáº¥u hÃ¬nh an toÃ n hÆ¡n</p>
                </div>
            </div>
            
            <!-- Medical Disclaimer -->
            <div style="background: white; padding: 20px; border-radius: 15px; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
                <p style="color: #dc2626; margin: 0; font-weight: 600; font-size: 16px;">
                    âš ï¸ LÆ¯U Ã Y KHOA QUAN TRá»ŒNG
                </p>
                <p style="color: #64748b; margin: 10px 0 0 0; line-height: 1.6;">
                    Há»‡ thá»‘ng Full Data AI nÃ y phÃ¢n tÃ­ch thÃ´ng tin tá»« <strong>toÃ n bá»™ tÃ i liá»‡u y khoa</strong> Ä‘Ã£ upload, 
                    nhÆ°ng chá»‰ mang tÃ­nh cháº¥t <strong>tham kháº£o</strong> vÃ  <strong>há»— trá»£</strong>.<br>
                    <strong style="color: #dc2626;">KHÃ”NG thay tháº¿</strong> cho viá»‡c khÃ¡m bá»‡nh, cháº©n Ä‘oÃ¡n vÃ  Ä‘iá»u trá»‹ trá»±c tiáº¿p tá»« Tháº§y thuá»‘c.<br>
                    <strong>Cáº¥p cá»©u y táº¿: Gá»i 115</strong> | <strong>Tham kháº£o Tháº§y thuá»‘c chuyÃªn khoa</strong> cho má»i váº¥n Ä‘á» sá»©c khá»e.
                </p>
            </div>
            
            <!-- Footer Links -->
            <div style="border-top: 1px solid #e2e8f0; padding-top: 20px; color: #94a3b8; font-size: 13px;">
                <p style="margin: 5px 0;">
                    ğŸ”’ Full Data Security | ğŸš€ Render Standard Optimized | ğŸ§  100% Coverage | ğŸ‡»ğŸ‡³ Made in Vietnam
                </p>
                <p style="margin: 5px 0;">
                    Â© 2024 Há»™i Tháº§y thuá»‘c tráº» Viá»‡t Nam. Full Data Medical AI System v3.0
                </p>
                <p style="margin: 10px 0 0 0;">
                    <a href="https://thaythuoctre.vn" target="_blank" style="color: #1d4ed8; text-decoration: none;">
                        ğŸŒ Website chÃ­nh thá»©c
                    </a> | 
                    <a href="mailto:info@thaythuoctre.vn" style="color: #1d4ed8; text-decoration: none;">
                        ğŸ“§ Há»— trá»£ ká»¹ thuáº­t
                    </a>
                </p>
            </div>
        </div>
        """)
        
        # EVENT HANDLERS
        submit_btn.click(ask_question, inputs=question_input, outputs=answer_output)
        question_input.submit(ask_question, inputs=question_input, outputs=answer_output)
        clear_btn.click(lambda: ("", ""), outputs=[question_input, answer_output])
    
    return interface

# Create full data interface
print("ğŸ¨ Creating Full Data Medical AI interface...")
interface = create_full_data_interface()

# Main execution
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ğŸš€ LAUNCHING FULL DATA MEDICAL AI FOR Há»˜I THáº¦Y THUá»C TRáºº VIá»†T NAM")
    print("=" * 70)
    print(f"ğŸ“¡ Server: 0.0.0.0:{port}")
    print(f"ğŸ”‘ API Key: {'âœ… Configured' if GOOGLE_API_KEY != 'dummy' else 'âŒ Missing'}")
    print(f"ğŸ’¾ Target: 100 files, 300MB total")
    print(f"ğŸ’» Hardware: Render Standard (1 CPU, 2GB RAM)")
    print(f"ğŸ¯ Coverage: 100% of uploaded files")
    print(f"âš¡ Optimizations:")
    print(f"   â€¢ Batch loading (3 files/batch)")
    print(f"   â€¢ Progressive vector DB creation")
    print(f"   â€¢ Memory monitoring & garbage collection")
    print(f"   â€¢ Intelligent page sampling")
    print(f"   â€¢ 1.6GB RAM safe limit")
    print("=" * 70)
    
    # Start full data initialization
    print("ğŸ”¥ Starting FULL DATA initialization...")
    print("ğŸ“Š This will process ALL files in the data folder")
    print("â±ï¸ Estimated time: 3-8 minutes depending on file count and sizes")
    
    init_thread = threading.Thread(target=initialize_system, daemon=True)
    init_thread.start()
    
    # Small delay for thread to start
    time.sleep(1.0)
    
    # Launch interface
    try:
        print("ğŸŒŸ Launching Full Data Medical AI interface...")
        interface.launch(
            server_name="0.0.0.0",
            server_port=port,
            share=False,
            show_error=True,
            show_api=False,
            quiet=False
        )
        
    except Exception as e:
        print(f"âŒ Primary launch failed: {e}")
        print("ğŸ”„ Attempting fallback launch...")
        
        try:
            interface.launch(
                server_name="0.0.0.0",
                server_port=port
            )
        except Exception as e2:
            print(f"âŒ Fallback launch failed: {e2}")
            print("ğŸ’” Unable to start server. Check configuration and try again.")
            
            # Emergency mode: Try with reduced functionality
            print("ğŸš¨ Attempting emergency mode with reduced data loading...")
            # Reset some global variables for emergency mode
            initialization_status = "ğŸš¨ Emergency mode - reduced data loading"
            try:
                interface.launch(
                    server_name="0.0.0.0",
                    server_port=port,
                    debug=True
                )
            except Exception as e3:
                print(f"âŒ Emergency mode also failed: {e3}")
                sys.exit(1)
