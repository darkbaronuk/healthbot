import os
import sys
import gradio as gr
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import shutil
import threading
import time
import gc
import psutil
from queue import Queue
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Setup port cho Render
port = int(os.environ.get("PORT", 7860))
print(f"üîç ENV PORT: {os.environ.get('PORT', 'Not set')}")
print(f"üîç Using port: {port}")

# Memory monitoring
def get_memory_usage():
    """Get current memory usage in MB"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def force_garbage_collection():
    """Force garbage collection to free memory"""
    gc.collect()
    time.sleep(0.1)  # Small delay for GC to complete

# Load environment variables
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "").strip()

if not GOOGLE_API_KEY or GOOGLE_API_KEY == "":
    print("‚ùå GOOGLE_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p!")
    GOOGLE_API_KEY = "dummy"
else:
    print(f"‚úÖ GOOGLE_API_KEY loaded: {len(GOOGLE_API_KEY)} chars")

print("üöÄ Kh·ªüi ƒë·ªông Full Data Medical AI cho H·ªôi Th·∫ßy thu·ªëc tr·∫ª Vi·ªát Nam...")
print(f"üíæ System RAM: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")
print(f"üíæ Available RAM: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f}GB")

# Global variables
qa_chain = None
vector_db = None
initialization_status = "‚öôÔ∏è ƒêang kh·ªüi t·∫°o h·ªá th·ªëng full data..."
system_ready = False
total_documents = 0
total_chunks = 0
processed_files = []
loading_progress = ""

def load_documents_in_batches(data_folder, batch_size=5):
    """Load documents in batches ƒë·ªÉ tr√°nh memory overflow"""
    global loading_progress, processed_files
    
    pdf_files = [f for f in os.listdir(data_folder) if f.endswith(".pdf")]
    pdf_files.sort()  # Sort ƒë·ªÉ c√≥ th·ª© t·ª± consistent
    
    print(f"üìÇ Found {len(pdf_files)} PDF files")
    print(f"üìÅ Files: {pdf_files[:10]}{'...' if len(pdf_files) > 10 else ''}")
    
    all_docs = []
    processed_files = []
    
    # Process files in batches
    for batch_start in range(0, len(pdf_files), batch_size):
        batch_end = min(batch_start + batch_size, len(pdf_files))
        batch_files = pdf_files[batch_start:batch_end]
        
        print(f"\nüì¶ Processing batch {batch_start//batch_size + 1}/{(len(pdf_files)-1)//batch_size + 1}")
        print(f"   Files: {batch_files}")
        
        loading_progress = f"Batch {batch_start//batch_size + 1}/{(len(pdf_files)-1)//batch_size + 1}: {batch_files[0]}..."
        
        batch_docs = []
        for file in batch_files:
            try:
                print(f"   üìÑ Loading: {file}")
                loader = PyPDFLoader(os.path.join(data_folder, file))
                file_docs = loader.load()
                
                # Intelligent page selection based on file size
                total_pages = len(file_docs)
                if total_pages <= 10:
                    # Small files: take all pages
                    selected_docs = file_docs
                elif total_pages <= 50:
                    # Medium files: take every other page
                    selected_docs = file_docs[::2]
                else:
                    # Large files: intelligent sampling
                    # Take first 10, middle 20, last 10
                    first_part = file_docs[:10]
                    middle_start = total_pages // 2 - 10
                    middle_part = file_docs[middle_start:middle_start + 20]
                    last_part = file_docs[-10:]
                    selected_docs = first_part + middle_part + last_part
                
                # Add metadata
                for i, doc in enumerate(selected_docs):
                    doc.metadata.update({
                        "source_file": file,
                        "original_page_count": total_pages,
                        "selected_page_count": len(selected_docs),
                        "file_index": batch_start + batch_files.index(file),
                        "page_in_selection": i
                    })
                
                batch_docs.extend(selected_docs)
                processed_files.append(file)
                print(f"   ‚úÖ {file}: {len(selected_docs)}/{total_pages} pages")
                
            except Exception as e:
                print(f"   ‚ùå Error loading {file}: {e}")
                continue
        
        all_docs.extend(batch_docs)
        
        # Memory check after each batch
        memory_mb = get_memory_usage()
        print(f"   üíæ Memory usage: {memory_mb:.1f}MB")
        
        if memory_mb > 1500:  # If using more than 1.5GB, force GC
            print("   üßπ High memory usage, forcing garbage collection...")
            force_garbage_collection()
            memory_after = get_memory_usage()
            print(f"   üíæ Memory after GC: {memory_after:.1f}MB")
        
        # Small delay between batches
        time.sleep(0.5)
    
    return all_docs

def create_chunks_with_memory_management(docs):
    """Create text chunks v·ªõi memory management"""
    print(f"\n‚úÇÔ∏è Creating chunks from {len(docs)} documents...")
    
    # Adaptive chunk size based on total document count
    if len(docs) > 500:
        chunk_size = 800
        chunk_overlap = 100
    elif len(docs) > 200:
        chunk_size = 1000
        chunk_overlap = 150
    else:
        chunk_size = 1200
        chunk_overlap = 200
    
    print(f"   üìè Chunk size: {chunk_size}, overlap: {chunk_overlap}")
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", "! ", "? ", ", ", " "]
    )
    
    # Process documents in batches to manage memory
    all_chunks = []
    batch_size = 50
    
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        print(f"   üì¶ Chunking batch {i//batch_size + 1}/{(len(docs)-1)//batch_size + 1}")
        
        batch_chunks = splitter.split_documents(batch)
        all_chunks.extend(batch_chunks)
        
        # Memory management
        if i % 100 == 0:  # Every 100 docs, check memory
            memory_mb = get_memory_usage()
            if memory_mb > 1400:
                force_garbage_collection()
    
    print(f"‚úÖ Created {len(all_chunks)} chunks total")
    return all_chunks

def create_vector_db_progressive(chunks, embedding, max_memory_mb=1600):
    """Create vector DB progressively ƒë·ªÉ avoid memory issues"""
    
    print(f"üíæ Creating vector DB with {len(chunks)} chunks...")
    print(f"üíæ Memory limit: {max_memory_mb}MB")
    
    # Determine batch size based on available memory
    memory_mb = get_memory_usage()
    available_memory = max_memory_mb - memory_mb
    
    if available_memory < 200:
        batch_size = 25
    elif available_memory < 400:
        batch_size = 50
    else:
        batch_size = 100
    
    print(f"üíæ Using batch size: {batch_size}")
    
    vector_db = None
    processed_chunks = 0
    
    try:
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(chunks) - 1) // batch_size + 1
            
            print(f"   üì¶ Processing vector batch {batch_num}/{total_batches} ({len(batch)} chunks)")
            
            if vector_db is None:
                # Create initial vector DB
                vector_db = Chroma.from_documents(
                    documents=batch,
                    embedding=embedding,
                    persist_directory=None
                )
            else:
                # Add to existing vector DB
                vector_db.add_documents(batch)
            
            processed_chunks += len(batch)
            
            # Memory monitoring
            memory_mb = get_memory_usage()
            print(f"   üíæ Memory: {memory_mb:.1f}MB, Processed: {processed_chunks}/{len(chunks)}")
            
            # Force GC if memory is high
            if memory_mb > max_memory_mb * 0.9:
                print("   üßπ High memory, forcing GC...")
                force_garbage_collection()
            
            # Small delay between batches
            time.sleep(0.2)
        
        print(f"‚úÖ Vector DB created successfully with {processed_chunks} chunks")
        return vector_db, 'success'
        
    except Exception as e:
        print(f"‚ùå Vector DB creation failed: {e}")
        return None, str(e)

def initialize_system():
    """Initialize system v·ªõi full data support cho Render Standard"""
    global qa_chain, vector_db, initialization_status, system_ready, total_documents, total_chunks, loading_progress
    
    start_time = time.time()
    
    print("\nüöÄ STARTING FULL DATA INITIALIZATION FOR RENDER STANDARD")
    print("=" * 60)
    print(f"üíæ Target: Load ALL files from data folder")
    print(f"üíæ Memory limit: 1.6GB (safe for 2GB system)")
    print(f"‚ö° Optimization: Batch processing + Intelligent sampling")
    print("=" * 60)
    
    try:
        # Step 1: Clean old data
        initialization_status = "üßπ Cleaning old data..."
        chroma_path = "chroma_db"
        if os.path.exists(chroma_path):
            shutil.rmtree(chroma_path)
        force_garbage_collection()
        
        # Step 2: Check data folder
        data_folder = "data"
        if not os.path.exists(data_folder):
            print(f"‚ùå Folder {data_folder} not found")
            initialization_status = "‚ùå Data folder not found"
            return False
        
        # Get folder info
        pdf_files = [f for f in os.listdir(data_folder) if f.endswith(".pdf")]
        if not pdf_files:
            print("‚ùå No PDF files found")
            initialization_status = "‚ùå No PDF files found"
            return False
        
        total_files = len(pdf_files)
        folder_size_mb = sum(os.path.getsize(os.path.join(data_folder, f)) for f in pdf_files) / 1024 / 1024
        
        print(f"üìÅ Found {total_files} PDF files ({folder_size_mb:.1f}MB total)")
        
        # Step 3: Load ALL documents in batches
        initialization_status = f"üìÇ Loading ALL {total_files} files in batches..."
        print(f"üìÇ Loading ALL {total_files} files with intelligent sampling...")
        
        docs = load_documents_in_batches(data_folder, batch_size=3)  # Smaller batch for safety
        
        if not docs:
            initialization_status = "‚ùå Failed to load any documents"
            return False
        
        total_documents = len(docs)
        print(f"‚úÖ Loaded {total_documents} pages from {len(processed_files)} files")
        
        # Step 4: Create chunks with memory management
        initialization_status = "‚úÇÔ∏è Creating chunks with memory management..."
        chunks = create_chunks_with_memory_management(docs)
        
        if not chunks:
            initialization_status = "‚ùå Failed to create chunks"
            return False
        
        total_chunks = len(chunks)
        print(f"‚úÖ Created {total_chunks} chunks")
        
        # Clear docs from memory
        del docs
        force_garbage_collection()
        
        # Step 5: Load embedding model
        initialization_status = "üîß Loading optimized embedding model..."
        print("üîß Loading memory-efficient embedding model...")
        
        try:
            embedding = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True, 'batch_size': 16}  # Smaller batch
            )
            print("‚úÖ Embedding model loaded")
        except Exception as e:
            print(f"‚ùå Embedding model failed: {e}")
            initialization_status = f"‚ùå Embedding error: {str(e)[:50]}..."
            return False
        
        # Step 6: Create vector database progressively
        initialization_status = "üíæ Building comprehensive vector database..."
        vector_db, status = create_vector_db_progressive(chunks, embedding)
        
        if status != 'success':
            initialization_status = f"‚ùå Vector DB error: {status[:50]}..."
            return False
        
        # Clear chunks from memory
        del chunks
        force_garbage_collection()
        
        # Step 7: Setup AI system
        if GOOGLE_API_KEY == "dummy":
            initialization_status = "‚ùå API Key not configured"
            return False
        
        initialization_status = "ü§ñ Setting up enhanced AI system..."
        print("ü§ñ Setting up Gemini AI with full data support...")
        
        try:
            prompt = PromptTemplate(
                template="""B·∫°n l√† tr·ª£ l√Ω y t·∫ø AI chuy√™n nghi·ªáp c·ªßa H·ªôi Th·∫ßy thu·ªëc tr·∫ª Vi·ªát Nam v·ªõi quy·ªÅn truy c·∫≠p v√†o c∆° s·ªü d·ªØ li·ªáu y khoa to√†n di·ªán.

C∆† S·ªû D·ªÆ LI·ªÜU: {total_files} files y khoa ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v·ªõi {total_chunks} knowledge chunks

T√ÄI LI·ªÜU THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {question}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
- Ph√¢n t√≠ch TO√ÄN DI·ªÜN th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu y khoa ƒë√£ ƒë∆∞·ª£c load ƒë·∫ßy ƒë·ªß
- T·ªïng h·ª£p ki·∫øn th·ª©c t·ª´ NHI·ªÄU ngu·ªìn t√†i li·ªáu y khoa ƒë√°ng tin c·∫≠y
- Tr·∫£ l·ªùi chi ti·∫øt, ch√≠nh x√°c b·∫±ng ti·∫øng Vi·ªát v·ªõi c·∫•u tr√∫c r√µ r√†ng
- Khi c√≥ ƒë·ªß th√¥ng tin trong database, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† c√≥ cƒÉn c·ª©
- N·∫øu th√¥ng tin ch∆∞a ƒë·∫ßy ƒë·ªß, n√≥i r√µ ƒëi·ªÅu n√†y v√† ƒë∆∞a ra ki·∫øn th·ª©c y khoa c∆° b·∫£n an to√†n
- Lu√¥n khuy·∫øn kh√≠ch tham kh·∫£o Th·∫ßy thu·ªëc chuy√™n khoa

ƒê·ªäNH D·∫†NG:
1. TR·∫¢ L·ªúI TR·ª∞C TI·∫æP
2. GI·∫¢I TH√çCH CHI TI·∫æT
3. KHUY·∫æN C√ÅO Y T·∫æ

TR·∫¢ L·ªúI:""".replace("{total_files}", str(len(processed_files))).replace("{total_chunks}", str(total_chunks)),
                input_variables=["context", "question"]
            )
            
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-pro",
                google_api_key=GOOGLE_API_KEY,
                temperature=0.2,
                max_output_tokens=8192
            )
            
            # Test API
            test_response = llm.invoke("Test")
            print(f"   ‚úÖ API test: {test_response.content[:30]}...")
            
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                retriever=vector_db.as_retriever(
                    search_type="mmr",
                    search_kwargs={
                        "k": 10,  # More chunks since we have full data
                        "lambda_mult": 0.7,
                        "fetch_k": 25
                    }
                ),
                chain_type_kwargs={"prompt": prompt},
                return_source_documents=True
            )
            
            print("‚úÖ Enhanced QA chain created")
            
        except Exception as e:
            print(f"‚ùå AI setup failed: {e}")
            initialization_status = f"‚ùå AI error: {str(e)[:50]}..."
            return False
        
        # Final memory cleanup
        force_garbage_collection()
        
        # Success!
        elapsed_time = time.time() - start_time
        final_memory = get_memory_usage()
        
        print("\n" + "=" * 60)
        print("‚úÖ FULL DATA SYSTEM INITIALIZATION COMPLETED!")
        print(f"üìä COMPREHENSIVE STATISTICS:")
        print(f"   ‚Ä¢ Total files processed: {len(processed_files)}")
        print(f"   ‚Ä¢ Total document pages: {total_documents}")
        print(f"   ‚Ä¢ Total knowledge chunks: {total_chunks}")
        print(f"   ‚Ä¢ Memory usage: {final_memory:.1f}MB")
        print(f"   ‚Ä¢ Initialization time: {elapsed_time:.1f}s")
        print(f"   ‚Ä¢ Vector DB: ‚úÖ Full data ready")
        print(f"   ‚Ä¢ AI Model: ‚úÖ Gemini Pro with 10-chunk retrieval")
        print(f"   ‚Ä¢ Coverage: üéØ 100% of uploaded files")
        print("=" * 60)
        
        initialization_status = f"‚úÖ FULL DATA READY! ({len(processed_files)} files, {total_chunks} chunks, {final_memory:.0f}MB)"
        system_ready = True
        loading_progress = f"‚úÖ Completed: {len(processed_files)} files processed"
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå INITIALIZATION FAILED: {e}")
        initialization_status = f"‚ùå Error: {str(e)[:100]}..."
        import traceback
        traceback.print_exc()
        return False

def ask_question(query):
    """Process questions v·ªõi full data support"""
    global initialization_status, system_ready
    
    if not query or not query.strip():
        return f"‚ùì Vui l√≤ng nh·∫≠p c√¢u h·ªèi.\n\nüìä Tr·∫°ng th√°i: {initialization_status}"
    
    query = query.strip()
    
    if len(query) > 2000:
        return "üìù C√¢u h·ªèi qu√° d√†i. Vui l√≤ng r√∫t ng·∫Øn d∆∞·ªõi 2000 k√Ω t·ª±."
    
    if GOOGLE_API_KEY == "dummy":
        return "üîë L·ªói API Key - H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh."
    
    if not system_ready or not qa_chain:
        return f"""üîß H·ªá th·ªëng ƒëang load TO√ÄN B·ªò d·ªØ li·ªáu...

üìä Tr·∫°ng th√°i: {initialization_status}
üìÅ Ti·∫øn ƒë·ªô: {loading_progress}

üí° Th√¥ng tin:
‚Ä¢ ƒêang x·ª≠ l√Ω TO√ÄN B·ªò files trong th∆∞ m·ª•c data
‚Ä¢ Th·ªùi gian ∆∞·ªõc t√≠nh: 3-8 ph√∫t (t√πy s·ªë l∆∞·ª£ng file)
‚Ä¢ H·ªá th·ªëng ƒë∆∞·ª£c t·ªëi ∆∞u cho Render Standard (2GB RAM)

üîÑ Vui l√≤ng ch·ªù h·ªá th·ªëng ho√†n t·∫•t vi·ªác load d·ªØ li·ªáu..."""
    
    try:
        print(f"üîç Processing query with FULL DATA: {query[:100]}...")
        
        start_time = time.time()
        result = qa_chain.invoke({"query": query})
        processing_time = time.time() - start_time
        
        answer = result.get("result", "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi.")
        sources = result.get("source_documents", [])
        
        # Enhanced source tracking
        if sources:
            source_files = {}
            for doc in sources:
                if "source_file" in doc.metadata:
                    file_name = doc.metadata["source_file"]
                    if file_name not in source_files:
                        source_files[file_name] = 0
                    source_files[file_name] += 1
            
            if source_files:
                answer += f"\n\nüìö **Ngu·ªìn tham kh·∫£o t·ª´ {len(source_files)} files:**\n"
                for i, (file, count) in enumerate(sorted(source_files.items()), 1):
                    answer += f"{i}. {file} ({count} references)\n"
        
        # Full system statistics
        current_memory = get_memory_usage()
        answer += f"\n\nüìä **Th·ªëng k√™ h·ªá th·ªëng FULL DATA:**\n"
        answer += f"‚Ä¢ Files ƒë√£ load: {len(processed_files)}\n"
        answer += f"‚Ä¢ T·ªïng chunks: {total_chunks}\n"
        answer += f"‚Ä¢ References t√¨m ƒë∆∞·ª£c: {len(sources)}\n"
        answer += f"‚Ä¢ Th·ªùi gian x·ª≠ l√Ω: {processing_time:.1f}s\n"
        answer += f"‚Ä¢ Memory usage: {current_memory:.0f}MB\n"
        answer += f"‚Ä¢ Coverage: üéØ 100% data ƒë∆∞·ª£c x·ª≠ l√Ω"
        
        answer += f"\n\n---\n‚ö†Ô∏è **L∆∞u √Ω:** Th√¥ng tin t·ª´ {len(processed_files)} files y khoa ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch. H√£y tham kh·∫£o Th·∫ßy thu·ªëc chuy√™n khoa ƒë·ªÉ ch·∫©n ƒëo√°n ch√≠nh x√°c. C·∫•p c·ª©u: 115."
        
        return answer
        
    except Exception as e:
        error_msg = str(e).lower()
        
        if "quota" in error_msg or "limit" in error_msg:
            return "‚ö†Ô∏è V∆∞·ª£t qu√° gi·ªõi h·∫°n API. Vui l√≤ng ch·ªù 1-2 ph√∫t v√† th·ª≠ l·∫°i."
        elif "memory" in error_msg:
            return "‚ö†Ô∏è H·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng th·ª≠ l·∫°i sau √≠t ph√∫t."
        else:
            return f"‚ùå L·ªói x·ª≠ l√Ω: {str(e)[:200]}... Vui l√≤ng th·ª≠ l·∫°i."

def create_full_data_interface():
    """Interface cho full data system"""
    
    with gr.Blocks(
        theme=gr.themes.Soft(),
        css="""
        .gradio-container { 
            background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); 
            font-family: 'Inter', sans-serif;
        }
        .custom-header {
            background: linear-gradient(135deg, #1e40af 0%, #1d4ed8 100%);
            color: white;
            padding: 35px;
            border-radius: 20px;
            margin-bottom: 30px;
            box-shadow: 0 12px 40px rgba(29, 78, 216, 0.25);
        }
        .info-card {
            background: white;
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 6px 20px rgba(0,0,0,0.08);
            border-left: 5px solid #1d4ed8;
            margin-bottom: 20px;
        }
        """,
        title="üè• Full Data Medical AI - H·ªôi Th·∫ßy thu·ªëc tr·∫ª Vi·ªát Nam"
    ) as interface:
        
        # HEADER
        gr.HTML("""
        <div class="custom-header">
            <div style="text-align: center;">
                <h1 style="margin: 0; font-size: 36px; font-weight: 800; color: white;">
                    üè• FULL DATA MEDICAL AI
                </h1>
                <p style="margin: 10px 0 0 0; font-size: 20px; color: white; opacity: 0.95;">
                    üöÄ Load 100% Files - Optimized for Render Standard
                </p>
                <p style="margin: 8px 0 0 0; font-size: 16px; color: white; opacity: 0.9;">
                    H·ªôi Th·∫ßy thu·ªëc tr·∫ª Vi·ªát Nam
                </p>
            </div>
            
            <div style="background: rgba(255,255,255,0.15); padding: 20px; border-radius: 15px; margin-top: 20px;">
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; text-align: center;">
                    <div>
                        <div style="font-size: 24px; margin-bottom: 5px;">üìÅ</div>
                        <strong style="color: white;">Full Coverage</strong><br>
                        <span style="color: #34d399;">100% Files Loaded</span>
                    </div>
                    <div>
                        <div style="font-size: 24px; margin-bottom: 5px;">üíæ</div>
                        <strong style="color: white;">Memory Optimized</strong><br>
                        <span style="color: #fbbf24;">2GB RAM Efficient</span>
                    </div>
                    <div>
                        <div style="font-size: 24px; margin-bottom: 5px;">‚ö°</div>
                        <strong style="color: white;">Smart Processing</strong><br>
                        <span style="color: #f87171;">Batch + Progressive</span>
                    </div>
                </div>
            </div>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(
                    lines=5,
                    placeholder="üí¨ V·ªõi c∆° s·ªü d·ªØ li·ªáu y khoa TO√ÄN DI·ªÜN, h√£y h·ªèi chi ti·∫øt v·ªÅ: b·ªánh l√Ω, thu·ªëc men, ch·∫©n ƒëo√°n, ƒëi·ªÅu tr·ªã, ph√≤ng ng·ª´a...",
                    label="ü©∫ C√¢u h·ªèi y t·∫ø (Full Data Support)",
                    max_lines=8,
                    info="H·ªá th·ªëng ƒë√£ load TO√ÄN B·ªò files - b·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ b·∫•t k·ª≥ ch·ªß ƒë·ªÅ y t·∫ø n√†o."
                )
                
                with gr.Row():
                    submit_btn = gr.Button("üîç T∆∞ v·∫•n Full Data AI", variant="primary", size="lg", scale=2)
                    clear_btn = gr.Button("üóëÔ∏è X√≥a", variant="secondary", scale=1)
            
            with gr.Column(scale=1):
                gr.HTML(f"""
                <div class="info-card">
                    <h3 style="color: #1e40af; margin: 0 0 15px 0;">üöÄ Full Data System</h3>
                    
                    <div style="margin-bottom: 15px;">
                        <strong style="color: #1e40af;">üìä Capacity:</strong><br>
                        <span style="color: #059669; font-size: 14px;">
                            ‚Ä¢ Target: 100 files, 300MB<br>
                            ‚Ä¢ Memory limit: 1.6GB<br>
                            ‚Ä¢ Batch processing: ‚úÖ<br>
                            ‚Ä¢ Progressive loading: ‚úÖ
                        </span>
                    </div>
                    
                    <div style="background: #f1f5f9; padding: 15px; border-radius: 10px; margin-bottom: 15px;">
                        <strong style="color: #1e40af;">üìä Status:</strong><br>
                        <span style="color: #059669; font-weight: 600; font-size: 14px;">
                            {initialization_status}
                        </span>
                    </div>
                    
                    <div style="background: #fef3c7; padding: 12px; border-radius: 8px;">
                        <strong style="color: #92400e;">‚ö° Progress:</strong><br>
                        <span style="color: #78350f; font-weight: 600; font-size: 14px;">
                            {loading_progress}
                        </span>
                    </div>
                </div>
                """)
        
        answer_output = gr.Textbox(
            lines=18,
            label="ü©∫ T∆∞ v·∫•n t·ª´ Full Data AI System",
            show_copy_button=True,
            interactive=False,
            placeholder="C√¢u tr·∫£ l·ªùi to√†n di·ªán t·ª´ h·ªá th·ªëng ƒë√£ load 100% d·ªØ li·ªáu s·∫Ω hi·ªÉn th·ªã ·ªü ƒë√¢y...",
            info="H·ªá th·ªëng ph√¢n t√≠ch t·ª´ TO√ÄN B·ªò files ƒë√£ upload v·ªõi ƒë·ªô ch√≠nh x√°c cao nh·∫•t."
        )
        
        # ENHANCED EXAMPLES for Full Data
        gr.Examples(
            examples=[
                "Ph√¢n t√≠ch to√†n di·ªán v·ªÅ b·ªánh ti·ªÉu ƒë∆∞·ªùng type 2: nguy√™n nh√¢n, tri·ªáu ch·ª©ng, ch·∫©n ƒëo√°n, ƒëi·ªÅu tr·ªã v√† bi·∫øn ch·ª©ng",
                "H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ cao huy·∫øt √°p: ph√¢n lo·∫°i, y·∫øu t·ªë nguy c∆°, ƒëi·ªÅu tr·ªã kh√¥ng d√πng thu·ªëc v√† d√πng thu·ªëc",
                "Thu·ªëc kh√°ng sinh: ph√¢n lo·∫°i, c∆° ch·∫ø t√°c d·ª•ng, nguy√™n t·∫Øc s·ª≠ d·ª•ng v√† t√¨nh tr·∫°ng kh√°ng thu·ªëc",
                "B·ªánh tim m·∫°ch: c√°c lo·∫°i b·ªánh, y·∫øu t·ªë nguy c∆°, ph√≤ng ng·ª´a v√† qu·∫£n l√Ω to√†n di·ªán",
                "S∆° c·ª©u c·∫•p c·ª©u: x·ª≠ l√Ω ƒë·ªôt qu·ªµ, nh·ªìi m√°u c∆° tim, s·ªëc ph·∫£n v·ªá v√† c√°c t√¨nh hu·ªëng nguy hi·ªÉm",
                "Vaccine v√† mi·ªÖn d·ªãch: l·ªãch ti√™m ch·ªßng, hi·ªáu qu·∫£ vaccine, t√°c d·ª•ng ph·ª• v√† ch·ªëng ch·ªâ ƒë·ªãnh",
                "B·ªánh truy·ªÅn nhi·ªÖm: HIV/AIDS, vi√™m gan B/C, lao ph·ªïi - ch·∫©n ƒëo√°n v√† ƒëi·ªÅu tr·ªã hi·ªán ƒë·∫°i",
                "S·ª©c kh·ªèe t√¢m th·∫ßn: tr·∫ßm c·∫£m, lo √¢u, r·ªëi lo·∫°n l∆∞·ª°ng c·ª±c - nh·∫≠n bi·∫øt v√† can thi·ªáp",
                "Dinh d∆∞·ª°ng l√¢m s√†ng: ƒë√°nh gi√° t√¨nh tr·∫°ng dinh d∆∞·ª°ng, can thi·ªáp dinh d∆∞·ª°ng ƒë·∫∑c bi·ªát",
                "B·ªánh l√Ω ph·ª• khoa: r·ªëi lo·∫°n kinh nguy·ªát, nhi·ªÖm tr√πng, u nang bu·ªìng tr·ª©ng",
                "Nhi khoa: ph√°t tri·ªÉn tr·∫ª em, b·ªánh th∆∞·ªùng g·∫∑p, l·ªãch kh√°m s·ª©c kh·ªèe ƒë·ªãnh k·ª≥",
                "L√£o khoa: c√°c h·ªôi ch·ª©ng l√£o h√≥a, ƒëa b·ªánh l√Ω, chƒÉm s√≥c ng∆∞·ªùi cao tu·ªïi",
                "Ung th∆∞: s√†ng l·ªçc, ch·∫©n ƒëo√°n s·ªõm, ƒëi·ªÅu tr·ªã ƒëa m√¥ th·ª©c v√† chƒÉm s√≥c gi·∫£m nh·∫π",
                "C·∫•p c·ª©u y khoa: ƒë√°nh gi√° ban ƒë·∫ßu, ph√¢n lo·∫°i m·ª©c ƒë·ªô kh·∫©n c·∫•p, x·ª≠ l√Ω ƒëa ch·∫•n th∆∞∆°ng",
                "Y h·ªçc d·ª± ph√≤ng: s√†ng l·ªçc b·ªánh, ti√™m ch·ªßng, gi√°o d·ª•c s·ª©c kh·ªèe c·ªông ƒë·ªìng"
            ],
            inputs=question_input,
            label="üí° C√¢u h·ªèi m·∫´u cho Full Data System - Test to√†n di·ªán",
            examples_per_page=10
        )
        
        # SYSTEM MONITORING SECTION
        gr.HTML("""
        <div style="background: white; padding: 20px; border-radius: 15px; margin-top: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
            <h4 style="color: #1e40af; margin: 0 0 15px 0;">üìä Full Data System Monitoring</h4>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px;">
                <div style="background: #f8fafc; padding: 15px; border-radius: 10px; border-left: 4px solid #059669;">
                    <strong style="color: #059669;">‚úÖ Data Coverage</strong><br>
                    <span style="color: #64748b; font-size: 14px;">
                        ‚Ä¢ 100% files trong th∆∞ m·ª•c data<br>
                        ‚Ä¢ Intelligent page sampling<br>
                        ‚Ä¢ Progressive chunk creation<br>
                        ‚Ä¢ Memory-optimized processing
                    </span>
                </div>
                <div style="background: #f8fafc; padding: 15px; border-radius: 10px; border-left: 4px solid #dc2626;">
                    <strong style="color: #dc2626;">üéØ Performance Optimizations</strong><br>
                    <span style="color: #64748b; font-size: 14px;">
                        ‚Ä¢ Batch loading (3 files/batch)<br>
                        ‚Ä¢ Progressive vector DB creation<br>
                        ‚Ä¢ Memory monitoring & GC<br>
                        ‚Ä¢ 1.6GB RAM limit protection
                    </span>
                </div>
                <div style="background: #f8fafc; padding: 15px; border-radius: 10px; border-left: 4px solid #1d4ed8;">
                    <strong style="color: #1d4ed8;">üöÄ Enhanced Retrieval</strong><br>
                    <span style="color: #64748b; font-size: 14px;">
                        ‚Ä¢ MMR algorithm v·ªõi 10 chunks<br>
                        ‚Ä¢ 25 candidate expansion<br>
                        ‚Ä¢ Source file tracking<br>
                        ‚Ä¢ Comprehensive statistics
                    </span>
                </div>
            </div>
        </div>
        """)
        
        # PROFESSIONAL FOOTER v·ªõi Full Data Info
        gr.HTML("""
        <div style="background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%); padding: 30px; border-radius: 20px; margin-top: 30px; border-top: 4px solid #1d4ed8; text-align: center;">
            <div style="margin-bottom: 25px;">
                <h4 style="margin: 0; color: #1e40af; font-size: 24px; font-weight: 700;">
                    üè• Full Data Medical AI System
                </h4>
                <p style="margin: 5px 0 0 0; color: #64748b; font-size: 16px;">
                    H·ªôi Th·∫ßy thu·ªëc tr·∫ª Vi·ªát Nam - Comprehensive Healthcare AI
                </p>
            </div>
            
            <!-- Technical Specifications -->
            <div style="background: white; padding: 25px; border-radius: 15px; margin-bottom: 25px; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
                <h5 style="color: #1e40af; margin: 0 0 20px 0; font-size: 18px; font-weight: 600;">
                    üîß Technical Specifications for 100 Files / 300MB
                </h5>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; text-align: left;">
                    <div style="background: #f1f5f9; padding: 15px; border-radius: 10px;">
                        <strong style="color: #1e40af;">üìÅ Data Processing:</strong><br>
                        <span style="color: #64748b; font-size: 14px;">
                            ‚Ä¢ Target: 100 files, 300MB total<br>
                            ‚Ä¢ Batch loading: 3 files per batch<br>
                            ‚Ä¢ Smart page sampling:<br>
                            &nbsp;&nbsp;- Small files (‚â§10 pages): All pages<br>
                            &nbsp;&nbsp;- Medium files (‚â§50 pages): Every 2nd page<br>
                            &nbsp;&nbsp;- Large files (>50 pages): First 10 + Middle 20 + Last 10<br>
                            ‚Ä¢ Progressive chunking v·ªõi memory protection
                        </span>
                    </div>
                    <div style="background: #f0fdf4; padding: 15px; border-radius: 10px;">
                        <strong style="color: #059669;">üíæ Memory Management:</strong><br>
                        <span style="color: #64748b; font-size: 14px;">
                            ‚Ä¢ Render Standard: 2GB RAM<br>
                            ‚Ä¢ Safe limit: 1.6GB usage<br>
                            ‚Ä¢ Auto garbage collection<br>
                            ‚Ä¢ Memory monitoring per batch<br>
                            ‚Ä¢ Progressive vector DB creation<br>
                            ‚Ä¢ Adaptive chunk sizing based on file count
                        </span>
                    </div>
                    <div style="background: #fef3c7; padding: 15px; border-radius: 10px;">
                        <strong style="color: #92400e;">‚ö° Performance Features:</strong><br>
                        <span style="color: #64748b; font-size: 14px;">
                            ‚Ä¢ MMR retrieval v·ªõi 10 chunks<br>
                            ‚Ä¢ 25 candidate expansion<br>
                            ‚Ä¢ Batch vector processing<br>
                            ‚Ä¢ Enhanced source tracking<br>
                            ‚Ä¢ Comprehensive statistics<br>
                            ‚Ä¢ 8K token output capacity
                        </span>
                    </div>
                </div>
            </div>
            
            <!-- Usage Recommendations -->
            <div style="background: white; padding: 20px; border-radius: 15px; margin-bottom: 25px; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
                <h5 style="color: #1e40af; margin: 0 0 15px 0; font-size: 16px; font-weight: 600;">
                    üìã Khuy·∫øn ngh·ªã s·ª≠ d·ª•ng v·ªõi 100 files
                </h5>
                <div style="text-align: left; color: #64748b; line-height: 1.6;">
                    <p style="margin: 10px 0;"><strong>‚úÖ T·ªëi ∆∞u:</strong> Upload files PDF c√≥ c·∫•u tr√∫c t·ªët, text r√µ r√†ng, √≠t h√¨nh ·∫£nh</p>
                    <p style="margin: 10px 0;"><strong>‚ö° Performance:</strong> Th·ªùi gian kh·ªüi t·∫°o: 3-8 ph√∫t t√πy s·ªë l∆∞·ª£ng v√† k√≠ch th∆∞·ªõc files</p>
                    <p style="margin: 10px 0;"><strong>üíæ Memory:</strong> H·ªá th·ªëng t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh batch size v√† chunk size theo t·∫£i</p>
                    <p style="margin: 10px 0;"><strong>üîÑ Restart:</strong> N·∫øu g·∫∑p l·ªói memory, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông restart v·ªõi c·∫•u h√¨nh an to√†n h∆°n</p>
                </div>
            </div>
            
            <!-- Medical Disclaimer -->
            <div style="background: white; padding: 20px; border-radius: 15px; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.05);">
                <p style="color: #dc2626; margin: 0; font-weight: 600; font-size: 16px;">
                    ‚ö†Ô∏è L∆ØU √ù Y KHOA QUAN TR·ªåNG
                </p>
                <p style="color: #64748b; margin: 10px 0 0 0; line-height: 1.6;">
                    H·ªá th·ªëng Full Data AI n√†y ph√¢n t√≠ch th√¥ng tin t·ª´ <strong>to√†n b·ªô t√†i li·ªáu y khoa</strong> ƒë√£ upload, 
                    nh∆∞ng ch·ªâ mang t√≠nh ch·∫•t <strong>tham kh·∫£o</strong> v√† <strong>h·ªó tr·ª£</strong>.<br>
                    <strong style="color: #dc2626;">KH√îNG thay th·∫ø</strong> cho vi·ªác kh√°m b·ªánh, ch·∫©n ƒëo√°n v√† ƒëi·ªÅu tr·ªã tr·ª±c ti·∫øp t·ª´ Th·∫ßy thu·ªëc.<br>
                    <strong>C·∫•p c·ª©u y t·∫ø: G·ªçi 115</strong> | <strong>Tham kh·∫£o Th·∫ßy thu·ªëc chuy√™n khoa</strong> cho m·ªçi v·∫•n ƒë·ªÅ s·ª©c kh·ªèe.
                </p>
            </div>
            
            <!-- Footer Links -->
            <div style="border-top: 1px solid #e2e8f0; padding-top: 20px; color: #94a3b8; font-size: 13px;">
                <p style="margin: 5px 0;">
                    üîí Full Data Security | üöÄ Render Standard Optimized | üß† 100% Coverage | üáªüá≥ Made in Vietnam
                </p>
                <p style="margin: 5px 0;">
                    ¬© 2024 H·ªôi Th·∫ßy thu·ªëc tr·∫ª Vi·ªát Nam. Full Data Medical AI System v3.0
                </p>
                <p style="margin: 10px 0 0 0;">
                    <a href="https://thaythuoctre.vn" target="_blank" style="color: #1d4ed8; text-decoration: none;">
                        üåê Website ch√≠nh th·ª©c
                    </a> | 
                    <a href="mailto:info@thaythuoctre.vn" style="color: #1d4ed8; text-decoration: none;">
                        üìß H·ªó tr·ª£ k·ªπ thu·∫≠t
                    </a>
                </p>
            </div>
        </div>
        """)
        
        # EVENT HANDLERS
        submit_btn.click(ask_question, inputs=question_input, outputs=answer_output)
        question_input.submit(ask_question, inputs=question_input, outputs=answer_output)
        clear_btn.click(lambda: ("", ""), outputs=[question_input, answer_output])
    
    return interface

# Create full data interface
print("üé® Creating Full Data Medical AI interface...")
interface = create_full_data_interface()

# Main execution
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("üöÄ LAUNCHING FULL DATA MEDICAL AI FOR H·ªòI TH·∫¶Y THU·ªêC TR·∫∫ VI·ªÜT NAM")
    print("=" * 70)
    print(f"üì° Server: 0.0.0.0:{port}")
    print(f"üîë API Key: {'‚úÖ Configured' if GOOGLE_API_KEY != 'dummy' else '‚ùå Missing'}")
    print(f"üíæ Target: 100 files, 300MB total")
    print(f"üíª Hardware: Render Standard (1 CPU, 2GB RAM)")
    print(f"üéØ Coverage: 100% of uploaded files")
    print(f"‚ö° Optimizations:")
    print(f"   ‚Ä¢ Batch loading (3 files/batch)")
    print(f"   ‚Ä¢ Progressive vector DB creation")
    print(f"   ‚Ä¢ Memory monitoring & garbage collection")
    print(f"   ‚Ä¢ Intelligent page sampling")
    print(f"   ‚Ä¢ 1.6GB RAM safe limit")
    print("=" * 70)
    
    # Start full data initialization
    print("üî• Starting FULL DATA initialization...")
    print("üìä This will process ALL files in the data folder")
    print("‚è±Ô∏è Estimated time: 3-8 minutes depending on file count and sizes")
    
    init_thread = threading.Thread(target=initialize_system, daemon=True)
    init_thread.start()
    
    # Small delay for thread to start
    time.sleep(1.0)
    
    # Launch interface
    try:
        print("üåü Launching Full Data Medical AI interface...")
        interface.launch(
            server_name="0.0.0.0",
            server_port=port,
            share=False,
            show_error=True,
            show_api=False,
            quiet=False
        )
        
    except Exception as e:
        print(f"‚ùå Primary launch failed: {e}")
        print("üîÑ Attempting fallback launch...")
        
        try:
            interface.launch(
                server_name="0.0.0.0",
                server_port=port
            )
        except Exception as e2:
            print(f"‚ùå Fallback launch failed: {e2}")
            print("üíî Unable to start server. Check configuration and try again.")
            
            # Emergency mode: Try with reduced functionality
            print("üö® Attempting emergency mode with reduced data loading...")
            # Reset some global variables for emergency mode
            initialization_status = "üö® Emergency mode - reduced data loading"
            try:
                interface.launch(
                    server_name="0.0.0.0",
                    server_port=port,
                    debug=True
                )
            except Exception as e3:
                print(f"‚ùå Emergency mode also failed: {e3}")
                sys.exit(1)
