import os
import gradio as gr
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader  # <-- Äá»”I SANG PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("Thiáº¿u OPENAI_API_KEY")

print(f"ðŸš€ Starting application...")

# Load vÃ  vector hÃ³a tÃ i liá»‡u
docs = []
data_folder = "data"
if os.path.exists(data_folder):
    print(f"ðŸ“‚ Scanning {data_folder} folder...")
    for file in os.listdir(data_folder):
        if file.endswith(".pdf"):
            print(f"ðŸ“„ Loading: {file}")
            try:
                loader = PyPDFLoader(os.path.join(data_folder, file))  # <-- Sá»¬A Äá»”I
                docs.extend(loader.load())
            except Exception as e:
                print(f"âŒ Error loading {file}: {e}")
    print(f"âœ… Loaded {len(docs)} documents")
else:
    print(f"âš ï¸ {data_folder} folder not found")

if docs:
    print("âœ‚ï¸ Splitting documents...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(docs)
    
    print("ðŸ”§ Creating embeddings...")
    embedding = OpenAIEmbeddings()
    vector_db = Chroma.from_documents(chunks, embedding, persist_directory="chroma_db")
    
    prompt = PromptTemplate(
        template="""
Báº¡n lÃ  trá»£ lÃ½ y táº¿ thÃ´ng minh. DÆ°á»›i Ä‘Ã¢y lÃ  ná»™i dung tÃ i liá»‡u Bá»™ Y táº¿:
{context}
CÃ¢u há»i: {question}
HÃ£y tráº£ lá»i báº±ng tiáº¿ng Viá»‡t ngáº¯n gá»n, chÃ­nh xÃ¡c.
""",
        input_variables=["context", "question"]
    )
    
    print("ðŸ¤– Setting up QA chain...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(
            model_name="gpt-4-turbo",
            max_tokens=200000,
            temperature=0.3
        ),
        retriever=vector_db.as_retriever(),
        chain_type_kwargs={"prompt": prompt}
    )
    
    def ask(query):
        try:
            return qa_chain.run(query)
        except Exception as e:
            return f"Lá»—i: {str(e)}. Vui lÃ²ng thá»­ cÃ¢u há»i ngáº¯n hÆ¡n."
else:
    def ask(query):
        return "ChÆ°a cÃ³ tÃ i liá»‡u PDF nÃ o Ä‘Æ°á»£c táº£i lÃªn."

# Láº¥y port tá»« biáº¿n mÃ´i trÆ°á»ng
port = int(os.environ.get("PORT", 7860))
print(f"ðŸš€ Starting server on port: {port}")

# Launch app
gr.Interface(
    fn=ask, 
    inputs=gr.Textbox(placeholder="Nháº­p cÃ¢u há»i y táº¿ cá»§a báº¡n..."),
    outputs=gr.Textbox(),
    title="ðŸ¥ Trá»£ lÃ½ Y táº¿ AI",
    description="Há»i Ä‘Ã¡p vá» y táº¿ dá»±a trÃªn tÃ i liá»‡u chÃ­nh thá»©c"
).launch(
    server_name="0.0.0.0",
    server_port=port,
    share=False,
    show_error=True
)
